{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9cID2o4B1hpw"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras import initializers\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "\n",
        "def _conv_layer(filters, kernel_size, strides=(1, 1), padding='same', name=None):\n",
        "    return Conv2D(filters, kernel_size, strides=strides, padding=padding,\n",
        "                  use_bias=True, kernel_initializer='he_normal', name=name)\n",
        "\n",
        "\n",
        "def _normalize_depth_vars(depth_k, depth_v, filters):\n",
        "    \"\"\"\n",
        "    Accepts depth_k and depth_v as either floats or integers\n",
        "    and normalizes them to integers.\n",
        "\n",
        "    Args:\n",
        "        depth_k: float or int.\n",
        "        depth_v: float or int.\n",
        "        filters: number of output filters.\n",
        "\n",
        "    Returns:\n",
        "        depth_k, depth_v as integers.\n",
        "    \"\"\"\n",
        "\n",
        "    if type(depth_k) == float:\n",
        "        depth_k = int(filters * depth_k)\n",
        "    else:\n",
        "        depth_k = int(depth_k)\n",
        "\n",
        "    if type(depth_v) == float:\n",
        "        depth_v = int(filters * depth_v)\n",
        "    else:\n",
        "        depth_v = int(depth_v)\n",
        "\n",
        "    return depth_k, depth_v\n",
        "\n",
        "\n",
        "class AttentionAugmentation2D(Layer):\n",
        "\n",
        "    def __init__(self, depth_k, depth_v, num_heads, relative=True, **kwargs):\n",
        "        \"\"\"\n",
        "        Applies attention augmentation on a convolutional layer\n",
        "        output.\n",
        "\n",
        "        Args:\n",
        "            depth_k: float or int. Number of filters for k.\n",
        "            Computes the number of filters for `v`.\n",
        "            If passed as float, computed as `filters * depth_k`.\n",
        "        depth_v: float or int. Number of filters for v.\n",
        "            Computes the number of filters for `k`.\n",
        "            If passed as float, computed as `filters * depth_v`.\n",
        "        num_heads: int. Number of attention heads.\n",
        "            Must be set such that `depth_k // num_heads` is > 0.\n",
        "        relative: bool, whether to use relative encodings.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: if depth_v or depth_k is not divisible by\n",
        "                num_heads.\n",
        "\n",
        "        Returns:\n",
        "            Output tensor of shape\n",
        "            -   [Batch, Height, Width, Depth_V] if\n",
        "                channels_last data format.\n",
        "            -   [Batch, Depth_V, Height, Width] if\n",
        "                channels_first data format.\n",
        "        \"\"\"\n",
        "        super(AttentionAugmentation2D, self).__init__(**kwargs)\n",
        "\n",
        "        if depth_k % num_heads != 0:\n",
        "            raise ValueError('`depth_k` (%d) is not divisible by `num_heads` (%d)' % (\n",
        "                depth_k, num_heads))\n",
        "\n",
        "        if depth_v % num_heads != 0:\n",
        "            raise ValueError('`depth_v` (%d) is not divisible by `num_heads` (%d)' % (\n",
        "                depth_v, num_heads))\n",
        "\n",
        "        if depth_k // num_heads < 1.:\n",
        "            raise ValueError('depth_k / num_heads cannot be less than 1 ! '\n",
        "                             'Given depth_k = %d, num_heads = %d' % (\n",
        "                             depth_k, num_heads))\n",
        "\n",
        "        if depth_v // num_heads < 1.:\n",
        "            raise ValueError('depth_v / num_heads cannot be less than 1 ! '\n",
        "                             'Given depth_v = %d, num_heads = %d' % (\n",
        "                                 depth_v, num_heads))\n",
        "\n",
        "        self.depth_k = depth_k\n",
        "        self.depth_v = depth_v\n",
        "        self.num_heads = num_heads\n",
        "        self.relative = relative\n",
        "\n",
        "        self.axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self._shape = input_shape\n",
        "\n",
        "        # normalize the format of depth_v and depth_k\n",
        "        self.depth_k, self.depth_v = _normalize_depth_vars(self.depth_k, self.depth_v,\n",
        "                                                           input_shape)\n",
        "\n",
        "        if self.axis == 1:\n",
        "            _, channels, height, width = input_shape\n",
        "        else:\n",
        "            _, height, width, channels = input_shape\n",
        "\n",
        "        if self.relative:\n",
        "            dk_per_head = self.depth_k // self.num_heads\n",
        "\n",
        "            if dk_per_head == 0:\n",
        "                print('dk per head', dk_per_head)\n",
        "\n",
        "            self.key_relative_w = self.add_weight(name='key_rel_w',\n",
        "                                                  shape=[2 * width - 1, dk_per_head],\n",
        "                                                  initializer=initializers.RandomNormal(\n",
        "                                                      stddev=dk_per_head ** -0.5))\n",
        "\n",
        "            self.key_relative_h = self.add_weight(name='key_rel_h',\n",
        "                                                  shape=[2 * height - 1, dk_per_head],\n",
        "                                                  initializer=initializers.RandomNormal(\n",
        "                                                      stddev=dk_per_head ** -0.5))\n",
        "\n",
        "        else:\n",
        "            self.key_relative_w = None\n",
        "            self.key_relative_h = None\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        if self.axis == 1:\n",
        "            # If channels first, force it to be channels last for these ops\n",
        "            inputs = K.permute_dimensions(inputs, [0, 2, 3, 1])\n",
        "\n",
        "        q, k, v = tf.split(inputs, [self.depth_k, self.depth_k, self.depth_v], axis=-1)\n",
        "\n",
        "        q = self.split_heads_2d(q)\n",
        "        k = self.split_heads_2d(k)\n",
        "        v = self.split_heads_2d(v)\n",
        "\n",
        "        # scale query\n",
        "        depth_k_heads = self.depth_k / self.num_heads\n",
        "        q *= (depth_k_heads ** -0.5)\n",
        "\n",
        "        # [Batch, num_heads, height * width, depth_k or depth_v] if axis == -1\n",
        "        qk_shape = [self._batch, self.num_heads, self._height * self._width, self.depth_k // self.num_heads]\n",
        "        v_shape = [self._batch, self.num_heads, self._height * self._width, self.depth_v // self.num_heads]\n",
        "        flat_q = K.reshape(q, K.stack(qk_shape))\n",
        "        flat_k = K.reshape(k, K.stack(qk_shape))\n",
        "        flat_v = K.reshape(v, K.stack(v_shape))\n",
        "\n",
        "        # [Batch, num_heads, HW, HW]\n",
        "        logits = tf.matmul(flat_q, flat_k, transpose_b=True)\n",
        "\n",
        "        # Apply relative encodings\n",
        "        if self.relative:\n",
        "            h_rel_logits, w_rel_logits = self.relative_logits(q)\n",
        "            logits += h_rel_logits\n",
        "            logits += w_rel_logits\n",
        "\n",
        "        weights = K.softmax(logits, axis=-1)\n",
        "        attn_out = tf.matmul(weights, flat_v)\n",
        "\n",
        "        attn_out_shape = [self._batch, self.num_heads, self._height, self._width, self.depth_v // self.num_heads]\n",
        "        attn_out_shape = K.stack(attn_out_shape)\n",
        "        attn_out = K.reshape(attn_out, attn_out_shape)\n",
        "        attn_out = self.combine_heads_2d(attn_out)\n",
        "        # [batch, height, width, depth_v]\n",
        "\n",
        "        if self.axis == 1:\n",
        "            # return to [batch, depth_v, height, width] for channels first\n",
        "            attn_out = K.permute_dimensions(attn_out, [0, 3, 1, 2])\n",
        "\n",
        "        attn_out.set_shape(self.compute_output_shape(self._shape))\n",
        "\n",
        "        return attn_out\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        output_shape = list(input_shape)\n",
        "        output_shape[self.axis] = self.depth_v\n",
        "        return tuple(output_shape)\n",
        "\n",
        "    def split_heads_2d(self, ip):\n",
        "        tensor_shape = K.shape(ip)\n",
        "\n",
        "        # batch, height, width, channels for axis = -1\n",
        "        tensor_shape = [tensor_shape[i] for i in range(len(self._shape))]\n",
        "\n",
        "        batch = tensor_shape[0]\n",
        "        height = tensor_shape[1]\n",
        "        width = tensor_shape[2]\n",
        "        channels = tensor_shape[3]\n",
        "\n",
        "        # Save the spatial tensor dimensions\n",
        "        self._batch = batch\n",
        "        self._height = height\n",
        "        self._width = width\n",
        "\n",
        "        ret_shape = K.stack([batch, height, width,  self.num_heads, channels // self.num_heads])\n",
        "        split = K.reshape(ip, ret_shape)\n",
        "        transpose_axes = (0, 3, 1, 2, 4)\n",
        "        split = K.permute_dimensions(split, transpose_axes)\n",
        "\n",
        "        return split\n",
        "\n",
        "    def relative_logits(self, q):\n",
        "        shape = K.shape(q)\n",
        "        # [batch, num_heads, H, W, depth_v]\n",
        "        shape = [shape[i] for i in range(5)]\n",
        "\n",
        "        height = shape[2]\n",
        "        width = shape[3]\n",
        "\n",
        "        rel_logits_w = self.relative_logits_1d(q, self.key_relative_w, height, width,\n",
        "                                               transpose_mask=[0, 1, 2, 4, 3, 5])\n",
        "\n",
        "        rel_logits_h = self.relative_logits_1d(\n",
        "            K.permute_dimensions(q, [0, 1, 3, 2, 4]),\n",
        "            self.key_relative_h, width, height,\n",
        "            transpose_mask=[0, 1, 4, 2, 5, 3])\n",
        "\n",
        "        return rel_logits_h, rel_logits_w\n",
        "\n",
        "    def relative_logits_1d(self, q, rel_k, H, W, transpose_mask):\n",
        "        rel_logits = tf.einsum('bhxyd,md->bhxym', q, rel_k)\n",
        "        rel_logits = K.reshape(rel_logits, [-1, self.num_heads * H, W, 2 * W - 1])\n",
        "        rel_logits = self.rel_to_abs(rel_logits)\n",
        "        rel_logits = K.reshape(rel_logits, [-1, self.num_heads, H, W, W])\n",
        "        rel_logits = K.expand_dims(rel_logits, axis=3)\n",
        "        rel_logits = K.tile(rel_logits, [1, 1, 1, H, 1, 1])\n",
        "        rel_logits = K.permute_dimensions(rel_logits, transpose_mask)\n",
        "        rel_logits = K.reshape(rel_logits, [-1, self.num_heads, H * W, H * W])\n",
        "        return rel_logits\n",
        "\n",
        "    def rel_to_abs(self, x):\n",
        "        shape = K.shape(x)\n",
        "        shape = [shape[i] for i in range(3)]\n",
        "        B, Nh, L, = shape\n",
        "        col_pad = K.zeros(K.stack([B, Nh, L, 1]))\n",
        "        x = K.concatenate([x, col_pad], axis=3)\n",
        "        flat_x = K.reshape(x, [B, Nh, L * 2 * L])\n",
        "        flat_pad = K.zeros(K.stack([B, Nh, L - 1]))\n",
        "        flat_x_padded = K.concatenate([flat_x, flat_pad], axis=2)\n",
        "        final_x = K.reshape(flat_x_padded, [B, Nh, L + 1, 2 * L - 1])\n",
        "        final_x = final_x[:, :, :L, L - 1:]\n",
        "        return final_x\n",
        "\n",
        "    def combine_heads_2d(self, inputs):\n",
        "        # [batch, num_heads, height, width, depth_v // num_heads]\n",
        "        transposed = K.permute_dimensions(inputs, [0, 2, 3, 1, 4])\n",
        "        # [batch, height, width, num_heads, depth_v // num_heads]\n",
        "        shape = K.shape(transposed)\n",
        "        shape = [shape[i] for i in range(5)]\n",
        "\n",
        "        a, b = shape[-2:]\n",
        "        ret_shape = K.stack(shape[:-2] + [a * b])\n",
        "        # [batch, height, width, depth_v]\n",
        "        return K.reshape(transposed, ret_shape)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'depth_k': self.depth_k,\n",
        "            'depth_v': self.depth_v,\n",
        "            'num_heads': self.num_heads,\n",
        "            'relative': self.relative,\n",
        "        }\n",
        "        base_config = super(AttentionAugmentation2D, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n",
        "\n",
        "def augmented_conv2d(ip, filters, kernel_size=(3, 3), strides=(1, 1),\n",
        "                     depth_k=0.2, depth_v=0.2, num_heads=8, relative_encodings=True):\n",
        "    \"\"\"\n",
        "    Builds an Attention Augmented Convolution block.\n",
        "\n",
        "    Args:\n",
        "        ip: keras tensor.\n",
        "        filters: number of output filters.\n",
        "        kernel_size: convolution kernel size.\n",
        "        strides: strides of the convolution.\n",
        "        depth_k: float or int. Number of filters for k.\n",
        "            Computes the number of filters for `v`.\n",
        "            If passed as float, computed as `filters * depth_k`.\n",
        "        depth_v: float or int. Number of filters for v.\n",
        "            Computes the number of filters for `k`.\n",
        "            If passed as float, computed as `filters * depth_v`.\n",
        "        num_heads: int. Number of attention heads.\n",
        "            Must be set such that `depth_k // num_heads` is > 0.\n",
        "        relative_encodings: bool. Whether to use relative\n",
        "            encodings or not.\n",
        "\n",
        "    Returns:\n",
        "        a keras tensor.\n",
        "    \"\"\"\n",
        "    # input_shape = K.int_shape(ip)\n",
        "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
        "\n",
        "    depth_k, depth_v = _normalize_depth_vars(depth_k, depth_v, filters)\n",
        "\n",
        "    conv_out = _conv_layer(filters - depth_v, kernel_size, strides)(ip)\n",
        "\n",
        "    # Augmented Attention Block\n",
        "    qkv_conv = _conv_layer(2 * depth_k + depth_v, (1, 1), strides)(ip)\n",
        "    attn_out = AttentionAugmentation2D(depth_k, depth_v, num_heads, relative_encodings)(qkv_conv)\n",
        "    attn_out = _conv_layer(depth_v, kernel_size=(1, 1))(attn_out)\n",
        "\n",
        "    output = concatenate([conv_out, attn_out], axis=channel_axis)\n",
        "    output = BatchNormalization()(output)\n",
        "    return output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBizI79M4X4u",
        "outputId": "4c74940b-c509-401d-dda9-71a41f43a9a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "\u001b[1m169001437/169001437\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.datasets import cifar100\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')\n",
        "x_train= x_train[:40000]\n",
        "y_train= y_train[:40000]\n",
        "x_test=x_test[:8000]\n",
        "y_test=y_test[:8000]\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "y_train = to_categorical(y_train, 100)\n",
        "y_test = to_categorical (y_test , 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RW8_zuHq6EpA",
        "outputId": "104b4f64-7c7c-463f-bd3a-94488cd1ecc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(40000, 32, 32, 3) 468.75 MB\n",
            "(8000, 32, 32, 3) 93.75 MB\n"
          ]
        }
      ],
      "source": [
        "print(x_train.shape, x_train.nbytes / 1024 / 1024, \"MB\")\n",
        "print(x_test.shape, x_test.nbytes / 1024 / 1024, \"MB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ePolH5Ly4bde"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, ReLU, GlobalAveragePooling2D, Dense\n",
        "\n",
        "def build_model():\n",
        "    ip = Input(shape=(32, 32, 3))\n",
        "\n",
        "    x = augmented_conv2d(ip, filters=64, kernel_size=(3, 3),\n",
        "                         depth_k=0.25, depth_v=0.25,\n",
        "                         num_heads=4, relative_encodings=True)\n",
        "\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = Conv2D(128, (3, 3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = ReLU()(x)\n",
        "\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    out = Dense(100, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=ip, outputs=out)\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVPDcvbCANxH",
        "outputId": "887b6bfb-6ce2-4b0b-f36a-15a732cd5eb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/7\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m348s\u001b[0m 276ms/step - accuracy: 0.0710 - loss: 4.1704 - val_accuracy: 0.1161 - val_loss: 3.8085\n",
            "Epoch 2/7\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m350s\u001b[0m 280ms/step - accuracy: 0.1506 - loss: 3.5998 - val_accuracy: 0.1570 - val_loss: 3.5691\n",
            "Epoch 3/7\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m384s\u001b[0m 282ms/step - accuracy: 0.1946 - loss: 3.3340 - val_accuracy: 0.1724 - val_loss: 3.5202\n",
            "Epoch 4/7\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m353s\u001b[0m 283ms/step - accuracy: 0.2237 - loss: 3.1811 - val_accuracy: 0.1727 - val_loss: 3.5679\n",
            "Epoch 5/7\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 282ms/step - accuracy: 0.2438 - loss: 3.0783 - val_accuracy: 0.1871 - val_loss: 3.4760\n",
            "Epoch 6/7\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 281ms/step - accuracy: 0.2567 - loss: 3.0003 - val_accuracy: 0.1919 - val_loss: 3.4331\n",
            "Epoch 7/7\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 278ms/step - accuracy: 0.2693 - loss: 2.9377 - val_accuracy: 0.2103 - val_loss: 3.3423\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x78a5622d2b50>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "model.fit(train_dataset, epochs=7, validation_data=test_dataset)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
